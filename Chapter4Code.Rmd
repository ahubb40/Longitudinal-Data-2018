---
title: 'Longitudinal Data: Fixed Covariates, Time-dependent Outcomes'
subtitle: 'Continuous Outcomes'
author: Alan Hubbard
date: Sept 11, 2018
#output: beamer_presentation
output: slidy_presentation
editor_options: 
  chunk_output_type: console
---

# Introduction {.bigger}
* For this chapter, we examine situations where:
    + Only “baseline” covariates
    + No expectation of any trends with time.
* Most straightforward way to analyze is to regress covariates against a summary statistic of the repeated outcome measures, for example:
\[
Y_i = \frac{1}{n_i} \sum_{j=1}^{n_i} Y_{i,j}
\]

# Theme of chapter
* Perform appropriate regression on the summary measure, for example:
\[
E(Y_i \mid \mathbf{X}_{i,1}) = \beta_0+\beta_1 X_{i,1,1}+\beta_2 X_{i,1,2}+\cdots++\beta_p X_{i,1,p}
\]
* If different subjects have different number of observations (i.e., the $n_i$ are not all the same), one might want to use weighted regression, e.g., the weight for each observation is ${wt}_i = n_i$.

# HIV Data Example

* Again the data on CD4 count and viral load for HIV+ patients
* We will fit models of form: $E(Y_i \mid X_i)=\beta_0+\beta_1 X_i$
where $Y_i$ is some function of all CD4 measures made on person $i$, e.g., the average of the $Y_{i,j}$ as above, and $X_i$ is the baseline viral load measurement.  We explore below after reading the data.

```{r load1,  warning=FALSE,message=FALSE}
# Load libraries and read in data
library(ggplot2)
library(data.table)
library(tidyverse)
dat=read.csv("cd4data.csv")
```

---
```{r datprint, warning=FALSE, message=FALSE, size="tiny"}
dat[10:30,]
```

# Data Manipulation via dplyr
* A big part of data analysis with longitudinal data is data manipulation.
* We already introduced the data.table functions in chapter 2, which can be used to manipulate, process, summarize data.
* We now introduce a powerful group of functions within the dplyr package.
* See <https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf> for dplry cheat sheet.

---

## Process and condense data into one row per id
```{r datamanip, size="tiny"}
# First, get rid of rows with etime < 0.
dat1=dplyr::filter(dat, etime >=  0)

## Use chaining (that is, apply functions in a sequence)
## that includes grouping data by id, create new variable
## (min_time) which is minimum time by id and then 
## ungroup to get back to original data with thie new 
## column
dat2 <- group_by(dat1, id) %>% mutate(base_time = min(etime)) %>% ungroup()
## Make elapsed time variable
dat3 <- mutate(dat2,elapse = etime-base_time) %>% mutate(logvl500 = log(vl500)) 
## Make sure data is ordered by time within id if not
## already
dat3 <- arrange(dat3, id, etime)
## Get the first observation of log(VL) for each id
## (called base_log_vl)
dat4 <- group_by(dat3, id) %>% mutate(base_log_vl=first( logvl500)) %>% ungroup()
## Get average CD4, total observations (our ni) by id and the observation within id (obs)
dat5 <- group_by(dat4, id) %>% mutate(tot_obs=n()) %>% mutate(ave_cd4 = mean(cd4,na.rm=T))  %>% mutate(obs = rank(elapse))%>% ungroup() %>% filter(obs==1)
## Look at data
tbl_df(dat5)
## Make sure all columns seen even don't fit on one
## screen
head(data.frame(dat5))
```

---

Now we show the a  smooth with average CD4 versus baseline log(VL)


```{r plot1, warning=FALSE}
p <- ggplot(data = dat5, aes(x = base_log_vl, y = ave_cd4))
p + geom_point()+geom_smooth()
```

---

# Linear Model
Our goal is to fit a linear model to predict average CD4, $Y_i$ give baseline log(VL), $X_i, i=1,...,m$ of form 
\[
E(Y_i \mid X_i)=\beta_0+\beta_1*X_i.
\]
First, we treat every observation equally (all given same weight)
```{r lm1, warning=FALSE}
lm1=lm(ave_cd4~base_log_vl, data=dat5,na.action = na.omit)
beta=coef(lm1)
se <- sqrt(diag(vcov(lm1)))
cc <- confint(lm1,"base_log_vl")
summary(lm1)

```

---

## Results Unweighted Logistic Regression

* The results show that $\hat{\beta_0}=$ `r round(beta[1],3)` and $\hat{\beta_1}=$ `r round(beta[2],3)` with the latter having standard error $SE=$ `r round(se[2])`.  
* The estimates and standard errors are used to derive a 95 percent confidence interval as $\hat{\beta_1} \pm 1.96*SE =$  `r round(beta[2],2)` $\pm 1.96*$ `r round(se[2],2)`  $= ($ `r round(beta[2]-1.96*se[2],2)` - `r round(beta[2]+1.96*se[2],2) `$) = ($ `r round(cc[1],2)` - `r round(cc[2],2)` $)$, the small differences due to rounding error.

---

## Results Weighted Logistic Regression
We repeat but now weight the observations depending on the number of observations used to estimate the mean (though other weights are sensible).  The justification is that the outcomes have different amounts of information, given the differences in the number of obsevation per person used to calculate them. First, we add the relative size of the points in our previous plot to look at their relative influence in weighted analysis.

---

```{r plot2, warning=FALSE}
p <- ggplot(data = dat5, aes(x = base_log_vl, y = ave_cd4))
p + geom_point(aes(size = tot_obs))
```

---

Now we fit linear regression
```{r lm2, warning=FALSE}
lm1=lm(ave_cd4~base_log_vl, data=dat5,na.action = na.omit,weights=tot_obs)
beta=coef(lm1)
se <- sqrt(diag(vcov(lm1)))
cc <- confint(lm1,"base_log_vl")
summary(lm1)

```

___

**Weighted Results Continued**


* The results show that $\hat{\beta_0}=$ `r round(beta[1],3)` and $\hat{\beta_1}=$ `r round(beta[2],3)` with the latter having standard error $SE=$ `r round(se[2])`.  
* The estimates and standard errors are used to derive a 95 percent confidence interval as $\hat{\beta_1} \pm 1.96*SE =$  `r round(beta[2],2)` $\pm 1.96*$ `r round(se[2],2)` $= ($ `r round(cc[1],2)` - `r round(cc[2],2)` $)$.

Now, we add the resulting fit to the ggplot and add the weights and size to the aes statement in the set-up command.

```{r plot3, warning=FALSE}
## First, a function (from )
p <- ggplot(data = dat5, aes(x = base_log_vl, y = ave_cd4,weight=tot_obs,size=tot_obs))
p + geom_point()+stat_smooth(method = "lm", col = "red",show.legend = F)
```

---
## Interpretation of results (for weighted regression)


* The high p-value indicates one can not reject the null of no association between baseline log(VL) and average future CD4.
* The estimated slope suggests a change in the mean of $\hat{\beta_1}=$ `r round(beta[2],3)` for a one unit change in log(VL).  Note that a one-unit change in log(VL) corresponds to a increase of a proportional increase of `r round(exp(1),2)`  in the orignal VL, e.g., a change of VL from 10000 to 27183 is a log(VL) change of 1.  
* The 95 \% CI  $= `r round(beta[2],2)` $\pm 1.96*$ `r round(se[2],2)` $= ($ `r round(cc[1],2)` - `r round(cc[2],2)` $)$, 
and normally one might think simple there is not enough information to make any conclusions about the associaition of average CD4 and baseline log(VL), but that's not the case here.  
* In this case, one unit log(VL) change is a relatively large change in the exposure, and that the resulting boundaries of the 95\% CI are relatively small negative (`r round(cc[1],2)`) and positive (`r round(cc[2],2)`) impacts.
* **Thus, there is little strong evidence of a lack of association, not just no evidence of a positive association** 
* Or we have both absence of evidence (p-value big) and evidence of absence (confidence interval small).


# Just need summary statistics of some (relevant) kind to summarize a longitudinal outcome vector, $Y_i$
* No objective best transform of repeated outcomes on individual to single summary measure.  
* Examples:
    + Median
    + Max or min
    + Variance
    + Estimated Slope of trend over time
    + Principle components
    + Etc.

# Converting repeated binary outcomes to a count (number of events)
* Repeated yes/no measurements on subjects.
* Interested in the association of baseline variables and the outcome of interest.
* Does not make sense to do if there are time-dependent covariates of interest.
* Summing the number of events and then  modeling counts as function of baseline covariates.



---

## Water Intervention Trial for HIV+ patients

* Subjects randomized to either device which filters out pathogens or a similar looking placebo devices.

* Subjects record daily whether or not they have a gastro-intestinal episode (yes/no)

* Purpose is to determine the amount of GI illness attributable to drinking water.

---

